# Training Config
## Training device
device: "cuda:7"
## Environment name / maximum episode length
env: cartpole
max_episode_len: 200
## Data collection and optimization loop
num_envs: 1
iteration_size: 1024
batch_size: 128
num_epochs: 15
max_training_steps: 100000
## Handling of timesteps
discount_factor: 0.99
## Optimizer
lr_actor: 0.0003
lr_critic: 0.001
## Action distribution
has_continuous_action_space: false
init_action_std: 0.6
min_action_std: 0.1
action_std_decay_rate: 0.05
action_std_decay_freq: 250000
# Model structure config
model_type: PPO
hidden_dim: &hidden_dim 64
# PPO config
eps_clip: 0.20

# Test
eval_freq: 10000
test_num_episodes: 100
test_max_episode_len: 200
test_gui: true

# Logging / Saving
log_freq: 1000
checkpoint_num: 10

# Domain randomization [domain_name, lower_bound, upper_bound, step_size] (lower, upper bounds are multipliers)
# domain_randomization:
#   - [masspole, 0.5, 1.51, 0.25]
#   - [length, 0.5, 1.51, 0.25]

# --- Model Structure ----
act: &act Tanh
actor_net: &actor_net
  - [Linear, auto, *hidden_dim, {}]
  - *act
  - [Linear, *hidden_dim, *hidden_dim, {}]
  - *act
  - [Linear, *hidden_dim, auto, {}]
  - [Softmax, {dim: -1}]
critic_net: &critic_net
  - [Linear, auto, *hidden_dim, {}]
  - *act
  - [Linear, *hidden_dim, *hidden_dim, {}]
  - *act
  - [Linear, *hidden_dim, auto, {}]

# --- Models ---
actor: *actor_net
critic: *critic_net