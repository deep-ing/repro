


# Action 
random_action : false
discount_factor : 0.9
abstract_dim : 2
buffer_len : 1000000
batch_size : 32
device : "cuda:0"

# Training Config
env : maze
n_envs : 4
<<<<<<< HEAD
timesteps : 1e5
target_update_freq : 1000
learning_rate : 0.0005
=======
timesteps : 1e6
target_update_freq : 1e4
learning_rate : 1e-4
>>>>>>> c7a22a99b4e050603b34151dd4d7325e0689481b
weight_decay : 0
horizon : 100

# Loss Magnitude
beta : 0.20
ld_lambda : 0.0
model_lambda : 0.0

# Exploration
epsilon_init : 0.9
epsilon_final : 0.1
epsilon_max_timesteps : 1e5

# Utils
log_freq : 100
checkpoint_num : 4 


# Test 
eval_freq : 1000
test_epsilon : 0.1
test_horizon : 100
test_episodes : 32


# --- Network Candidates ----
act: &act Tanh
encoder_: &encoder_
  - [Conv2d, auto, 8, { kernel_size: 2, stride: 1, padding: same}]
  - *act
  - [Conv2d, 8, 16, { kernel_size: 2, stride: 1, padding: same}]
  - *act
  - [MaxPool2d, { kernel_size: 2 }]
  - [Conv2d, 16, 32, { kernel_size: 3, stride: 1, padding: same}]
  - *act
  - [MaxPool2d, { kernel_size: 2 }]
  - [Flatten, { }]
  - [Linear, 4608, 200, {}]
  - *act
  - [Linear, 200, 100, {}]
  - *act
  - [Linear, 100, 50, {}]
  - *act
  - [Linear, 50, 10, {}]
  - *act
  - [Linear, 10, auto, {}]

q : &q
  - [Linear, auto, 20, {}]
  - *act
  - [Linear, 20, 50, {}]
  - *act
  - [Linear, 50, 20, {}]
  - *act
  - [Linear, 20, auto, {}]

transition: &transition
  - [Linear, auto, 10, {}]
  - *act
  - [Linear, 10, 30, {}]
  - *act
  - [Linear, 30, 30, {}]
  - *act
  - [Linear, 30, 10, {}]
  - *act
  - [Linear, 10, auto, {}]

reward_gamma : &reward_gamma
  - [Linear, auto, 10, {}]
  - *act
  - [Linear, 10, 50, {}]
  - *act
  - [Linear, 50, 20, {}]
  - *act
  - [Linear, 20, auto, {}]


# --- Models ---
encoder: *encoder_
q_net: *q
transition_net: *transition
reward_net: *reward_gamma
discount_net: *reward_gamma

